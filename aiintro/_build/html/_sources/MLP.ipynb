{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Multi-Layer-Perceptron\" data-toc-modified-id=\"Multi-Layer-Perceptron-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Multi Layer Perceptron</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notations-and-Basic-Characteristics\" data-toc-modified-id=\"Notations-and-Basic-Characteristics-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Notations and Basic Characteristics</a></span></li><li><span><a href=\"#Early-MLP-Example:-Autonomos-Driving\" data-toc-modified-id=\"Early-MLP-Example:-Autonomos-Driving-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Early MLP Example: Autonomos Driving</a></span></li><li><span><a href=\"#Architecture\" data-toc-modified-id=\"Architecture-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Architecture</a></span><ul class=\"toc-item\"><li><span><a href=\"#Number-of-hidden-layers\" data-toc-modified-id=\"Number-of-hidden-layers-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Number of hidden layers</a></span></li><li><span><a href=\"#Activation--and-Loss-functions\" data-toc-modified-id=\"Activation--and-Loss-functions-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Activation- and Loss-functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Activation-functions-in-hidden-layers\" data-toc-modified-id=\"Activation-functions-in-hidden-layers-1.3.2.1\"><span class=\"toc-item-num\">1.3.2.1&nbsp;&nbsp;</span>Activation functions in hidden layers</a></span></li><li><span><a href=\"#Activation-functions-in-the-output-layer-and-loss-functions\" data-toc-modified-id=\"Activation-functions-in-the-output-layer-and-loss-functions-1.3.2.2\"><span class=\"toc-item-num\">1.3.2.2&nbsp;&nbsp;</span>Activation functions in the output layer and loss functions</a></span></li></ul></li></ul></li><li><span><a href=\"#Gradient-Descent-Learning\" data-toc-modified-id=\"Gradient-Descent-Learning-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Gradient Descent Learning</a></span></li></ul></li><li><span><a href=\"#MLP-Implementation-and-Demonstration\" data-toc-modified-id=\"MLP-Implementation-and-Demonstration-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>MLP Implementation and Demonstration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definition-of-helper-functions\" data-toc-modified-id=\"Definition-of-helper-functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Definition of helper functions</a></span></li><li><span><a href=\"#Implementation-of-Single-Forward--and-Backward-Pass-in-MLP\" data-toc-modified-id=\"Implementation-of-Single-Forward--and-Backward-Pass-in-MLP-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Implementation of Single Forward- and Backward Pass in MLP</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-into-$K=3$-classes\" data-toc-modified-id=\"Classification-into-$K=3$-classes-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Classification into $K=3$ classes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-Pass:\" data-toc-modified-id=\"Forward-Pass:-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>Forward-Pass:</a></span></li><li><span><a href=\"#Contribution-to-Loss-Function\" data-toc-modified-id=\"Contribution-to-Loss-Function-2.2.1.2\"><span class=\"toc-item-num\">2.2.1.2&nbsp;&nbsp;</span>Contribution to Loss Function</a></span></li><li><span><a href=\"#Backward-Pass\" data-toc-modified-id=\"Backward-Pass-2.2.1.3\"><span class=\"toc-item-num\">2.2.1.3&nbsp;&nbsp;</span>Backward Pass</a></span></li><li><span><a href=\"#Forward-Pass-with-adapted-weights\" data-toc-modified-id=\"Forward-Pass-with-adapted-weights-2.2.1.4\"><span class=\"toc-item-num\">2.2.1.4&nbsp;&nbsp;</span>Forward-Pass with adapted weights</a></span></li><li><span><a href=\"#New-Contribution-to-Loss-Function\" data-toc-modified-id=\"New-Contribution-to-Loss-Function-2.2.1.5\"><span class=\"toc-item-num\">2.2.1.5&nbsp;&nbsp;</span>New Contribution to Loss Function</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Multilayer-Perceptron-(MLP)-for-handwritten-digit-recognition\" data-toc-modified-id=\"Multilayer-Perceptron-(MLP)-for-handwritten-digit-recognition-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Multilayer Perceptron (MLP) for handwritten digit recognition</a></span><ul class=\"toc-item\"><li><span><a href=\"#Class-MLP\" data-toc-modified-id=\"Class-MLP-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Class MLP</a></span></li><li><span><a href=\"#Load-labeled-data\" data-toc-modified-id=\"Load-labeled-data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Load labeled data</a></span></li><li><span><a href=\"#Split-labeled-dataset-into-training--and-test-partition\" data-toc-modified-id=\"Split-labeled-dataset-into-training--and-test-partition-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Split labeled dataset into training- and test-partition</a></span></li><li><span><a href=\"#Generate,-configure,-train-and-test-MLP\" data-toc-modified-id=\"Generate,-configure,-train-and-test-MLP-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Generate, configure, train and test MLP</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron\n",
    "This notebook is based on the theory on Neural Networks as described in [notebook SLP](SLP.ipynb). Since Multi Layer Perceptrons (MLPs) are an extension of SLPs, it is strongly recommended to first read [notebook SLP](SLP.ipynb).\n",
    "\n",
    "## Notations and Basic Characteristics\n",
    "A Multi Layer Perceptron (MLP) with $L\\geq 2$ layers is a Feedforward Neural Network (FNN), which consists of \n",
    "* an input-layer (which is actually not counted as *layer*)\n",
    "* an output layer \n",
    "* a sequence of $L-1$ hidden layers inbetween the input- and output-layer\n",
    "\n",
    "Usually the number of hidden layers is 1,2 or 3. All neurons of a layer are connected to all neurons of the successive layer. A layer with this property is also called a **fully-connected layer** or a **dense layer**. \n",
    "\n",
    "An example of a $L=3$ layer MLP is shown in the following picture. \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/mlpL3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of SLPs, the biases in MLP can be modelled implicitily by including to all non-output-layers a constant neuron $x_0=1$, or by the explicit bias-vector $\\mathbf{b^l}$ in layer $l$. In the picture above, the latter option is applied.\n",
    "\n",
    "In order to provide a unified description the following notation is used:\n",
    "* the number of neurons in layer $l$ is denoted by $z_l$.\n",
    "* the output of the layer in depth $l$ is denoted by the vector $\\mathbf{h^l}=(h_1^l,h_2^l,\\ldots,h_{z_l}^l)$, \n",
    "* $\\mathbf{x}=\\mathbf{h^0}$ is the input to the network,\n",
    "* $\\mathbf{y}=\\mathbf{h^L}$ is the network's output,\n",
    "* $\\mathbf{b^l}$ is the bias-vector of layer $l$,\n",
    "* $W^l$ is the weight-matrix of layer $l$. It's entry $W_{ij}^l$ is the weight from the j.th neuron in layer $l-1$ to the i.th neuron in layer $l$. Hence, the weight-matrix $W^l$ has $z_l$ rows and $z_{l-1}$ columns.\n",
    "\n",
    "With this notation the **Forward-Pass** of the MLP in the picture above can be calculated as follows:\n",
    "\n",
    "**Output of first hidden-layer:**\n",
    "$$\\left( \\begin{array}{c} h_1^1 \\\\ h_2^1 \\\\ h_3^1 \\\\ h_4^1 \\end{array} \\right) = g\\left( \\left( \\begin{array}{ccc} W_{11}^1 & W_{12}^1 & W_{13}^1 \\\\ W_{21}^1 & W_{22}^1 & W_{23}^1 \\\\ W_{31}^1 & W_{32}^1 & W_{33}^1 \\\\ W_{41}^1 & W_{42}^1 & W_{43}^1 \\end{array} \\right) \\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\end{array} \\right) + \\left( \\begin{array}{c} b_1^1 \\\\ b_2^1 \\\\ b_3^1 \\\\ b_4^1 \\end{array} \\right) \\right)$$\n",
    "\n",
    "\n",
    "\n",
    "**Output of second hidden-layer:**\n",
    "\n",
    "$$\\left( \\begin{array}{c} h_1^2 \\\\ h_2^2 \\\\ h_3^2 \\end{array} \\right) = g\\left( \\left( \\begin{array}{cccc} W_{11}^2 & W_{12}^2 & W_{13}^2 & W_{14}^2\\\\ W_{21}^2 & W_{22}^2 & W_{23}^2 & W_{24}^2\\\\ W_{31}^2 & W_{32}^2 & W_{33}^2 & W_{34}^2 \\end{array} \\right) \\left( \\begin{array}{c} h^1_1 \\\\ h^1_2 \\\\ h^1_3 \\\\ h^1_4 \\end{array} \\right) + \\left( \\begin{array}{c} b_1^2 \\\\ b_2^2 \\\\ b_3^2 \\end{array} \\right) \\right)$$\n",
    "\n",
    "**Output of the network:**\n",
    "\n",
    "$$y = \\left( \\begin{array}{c} h_1^3 \\\\ \\end{array} \\right) = g\\left( \\left( \\begin{array}{ccc} W_{11}^3 & W_{12}^3 & W_{13}^3 \\end{array} \\right) \\left( \\begin{array}{c} h^2_1 \\\\ h^2_2 \\\\ h^2_3 \\end{array} \\right) + \\left( \\begin{array}{c} b_1^3 \\end{array} \\right) \\right)$$\n",
    "\n",
    "As in the case of [Single Layer Perceptrons](SLP.ipynb) the three categories regression, binary- and $K$-ary classification are distinguished. In contrast to SLPs, MLPs are able to **learn non-linear** models. This difference is depicted below: The left hand side shows the linear classification-boundary, as learned by a SLP, whereas on the right-hand side the non-linear boundary, as learned by a MLP from the same training data, is plotted.  \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/nonlinearClassification.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early MLP Example: Autonomos Driving\n",
    "The ALVINN net is a MLP with one hidden layer. It has been designed and trained for *road following* in autonomous driving. The input has been provided by a simple $30 \\times 32$ greyscale camera. As shown in the picture below, the hidden layer contains only 4 neurons. In the output-layer each of the 30 neurons belongs to one \"steering-wheel-direction\". The training data has been collected by recording videos while an expert driver steers the car. For each frame (input) the steering-wheel-direction (label) has been tracked. \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/alvinnNN.jpg\" style=\"height:400px\"/>\n",
    "\n",
    "After training the vehicle cruised autonomously for 90 miles on a highway at a speed of up to 70mph. The test-highway has not been included in the training cruises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "### Number of hidden layers\n",
    "In the design of MLPs, the number of required hidden layers and the number of neurons per hidden layer are crucial hyperparameters, which strongly influence the network's performance. Appropriate values for these parameters strongly depend on the application and data at hand. They can not be calculated analytically, but have to be determined in corresponding evaluation- and optimization experiments.\n",
    "\n",
    "In order to roughly determine ranges for a suitable number of hidden neurons, one should consider, that an increasing number of hidden neurons \n",
    "\n",
    "* requires more training data to learn a robust model, since more parameters must be learned,\n",
    "* allows to learn more complex models,\n",
    "* increases the risk of overfitting.\n",
    "\n",
    "### Activation- and Loss-functions\n",
    "#### Activation functions in hidden layers\n",
    "The type of activation function to be used in the hidden-layers of a MLP is an hyperparameter, which must be configured by the user, i.e. it is not determined by e.g. the application category. Typical activations for the hidden-layers are:\n",
    "\n",
    "* sigmoid\n",
    "* tanh\n",
    "* relu\n",
    "* leaky relu\n",
    "\n",
    "Finding the best, or at least an appropriate, activation function for the application and data at hand requires empirical analysis.\n",
    "#### Activation functions in the output layer and loss functions\n",
    "The configuration of the activation function in the output-layer and the loss function, which is minimized in the training-stage, depend on the application-category in the same way as in the [SLP](SLP.ipynb):\n",
    "\n",
    "**Regression:**\n",
    "* Number of neurons in the output-layer: 1\n",
    "* Activation function in the output-layer: identity \n",
    "* Loss Function: Sum of Squared Errors (SSE)\n",
    "\n",
    "**Binary Classification:**\n",
    "* Number of neurons in the output-layer: 1\n",
    "* Activation function in the output-layer: sigmoid \n",
    "* Loss Function: binary Cross-Entropy\n",
    "\n",
    "**$K$-ary Classification:**\n",
    "* Number of neurons in the output-layer: K\n",
    "* Activation function in the output-layer: softmax \n",
    "* Loss Function: Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Learning\n",
    "For training, MLPs apply the same approach as SLPs: Gradient Descent. The general consept of Gradient Descent learning is:\n",
    "1. Define a **Loss Function** $E(T,\\Theta)$, which somehow measures the deviation between the current network $\\mathbf{y}$ output and the target output $\\mathbf{r}$.\n",
    "2. Calculate the gradient of the Loss Function: $$\\nabla E(T,\\Theta) = \\left( \\begin{array}{c}  \\frac{\\partial E}{\\partial W^l_{1,0}} \\\\ \\frac{\\partial E}{\\partial W_{1,1}^l} \\\\ \\vdots \\\\  \\frac{\\partial E}{\\partial W^l_{K,d+1}} \\end{array} \\right). $$\n",
    "3. Adapt all parameters into the direction of the negative gradient. This weight adaptation guarantees that the Loss Function is iteratively minimized.: $$W^l_{i,j}=W^l_{i,j}+\\Delta W^l_{i,j} = W^l_{i,j}+\\eta \\cdot -\\frac{\\partial E}{\\partial W^l_{i,j}},$$ where $\\eta$ is the important hyperparameter **learning rate**. \n",
    "\n",
    "This approach is described in detail in [notebook SLP](SLP.ipynb). For the MLP, here we just present the *Backward Pass* weight adaptation-rule, resulting from the aforementioned Gradient Descent approach. The algorithm is denoted **Backpropagation Algorithm**.\n",
    "\n",
    "The weight-matrix $W^l$ in layer $l$, with $l \\in \\lbrace 1,\\ldots,L\\rbrace$, is adapted in each iteration by \n",
    "$$W^l=W^l+ \\Delta W^l,$$\n",
    "where\n",
    "$$\n",
    "\\Delta W^l = \\eta \\sum\\limits_{t=1}^N \\boldsymbol{D}_t^l * (\\mathbf{h}_t^{l-1})^T\n",
    "$$\n",
    "for Gradient Descent Batch-Learning, and\n",
    "$$\n",
    "\\Delta W^l = \\eta \\boldsymbol{D}_t^l * (\\mathbf{h}_t^{l-1})^T\n",
    "$$\n",
    "for Stochastic Gradient Descent (SGD) Online-Learning. In this adaptation formulas\n",
    "\n",
    "* $\\mathbf{h}_t^{l-1}$ is the output-vector at layer $l-1$, if the t.th training-element $\\mathbf{x}_t$ is at the input of the MLP,\n",
    "* the matrix $\\boldsymbol{D}_t^l$ is calculated recursively as follows:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|}\n",
    "\t\t\\hline\n",
    "\t\tlayer \\; l & \\boldsymbol{D}_t^l \\\\\n",
    "\t\t\\hline\n",
    "\t\tL & \\boldsymbol{\\Delta}_t \\\\\n",
    "\t\tL-1 & \\left( (W^{L})^T * \\boldsymbol{\\Delta}_t \\right) \\cdot g'(W^{L-1}\\mathbf{h}_t^{L-2}) \\\\\n",
    "\t\tL-2 & \\left((W^{L-1})^T * \\boldsymbol{D}_t^{L-1} \\right) \\cdot g'(W^{L-2}\\mathbf{h}_t^{L-3}) \\\\\n",
    "\t\t\\vdots & \\vdots \\\\\n",
    "\t\tl & \\left((W^{l+1})^T * \\boldsymbol{D}_t^{l+1} \\right) \\cdot g'(W^{l}\\mathbf{h}_t^{l-1}) \\\\\n",
    "        \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "where:\n",
    "* $*$ denotes matrix-multiplication\n",
    "* $\\cdot$ denotes elementwise-multiplication\n",
    "* $g'()$ is the first derivation of the activation function applied in layer $l$.\n",
    "* the error-vector $\\Delta_t$ is as defined in [notebook SLP](SLP.ipynb):\n",
    "\n",
    "$$\\boldsymbol{\\Delta}_t=\\left( \\begin{array}{c} \\Delta_{t,1} \\\\ \\Delta_{t,2} \\\\ \\vdots \\\\ \\Delta_{t,z_L} \\end{array} \\right) = \\left( \\begin{array}{c} r_{t,1} - h^L_{t,1} \\\\ r_{t,2} - h^L_{t,2} \\\\ \\vdots \\\\ r_{t,z_L} - h^L_{t,z_L} \\end{array} \\right)$$\n",
    "\n",
    "Note that in the calculation of $\\boldsymbol{D}_t^l$ depends on the weight-matrices $W^{l+1},\\ldots W^{L}$. It is important that for this the old weight-matrices (before the update in the current iteration) are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation and Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T17:09:06.442000Z",
     "start_time": "2017-12-06T18:08:59.762000+01:00"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4,suppress=True)\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T17:09:06.454000Z",
     "start_time": "2017-12-06T18:09:06.444000+01:00"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#Activation functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Softmax activation function.\"\"\"\n",
    "    return np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "\n",
    "def identity(z):\n",
    "    \"\"\"Identity activation function.\"\"\"\n",
    "    return z\n",
    "\n",
    "#Derivative of sigmoid function\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def oneToMany(j,d=10):\n",
    "    \"\"\"Return a d-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((d, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "#Loss functions and performance metrics\n",
    "def mse(r,y):\n",
    "    return 1.0/len(r)*np.sqrt((r-y)**2)\n",
    "\n",
    "def accuracy(r,y):\n",
    "    return np.count_nonzero(r==y)/float(len(r))\n",
    "\n",
    "def crossEntropy(r,y):\n",
    "    return np.sum(np.sum(r*np.log2(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Single Forward- and Backward Pass in MLP\n",
    "As in the corresponding section of the [SLP notebook](SLP.ipynb), this subsection demonstrates\n",
    "* the forward-pass, i.e. the calculation of the MLP-output from a given input and given weight-matrices\n",
    "* the backward-pass, i.e the adaptation of the weight-matrices in dependence of the current error-vector at the output of the MLP.\n",
    "\n",
    "In order to keep this demo as simple as possible only one training element is applied.\n",
    "\n",
    "### Classification into $K=3$ classes\n",
    "\n",
    "For this demonstration the MLP architecture of the following picture is applied. The task of this network is to classify input observations with $d=3$ features into one of $K=3$ classes.\n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/mlpL2K3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "The single hidden layer applies a sigmoid- and the output-layer a softmax-activation. In the picture above explicit biases are drawn. In the following demonstrations, the biases are implemented by the constant $x_0=1$ and the first column of the weight-matrices.\n",
    "\n",
    "Define an input-vector with 4 elements (the first element is the constant bias $x_0=1$). The target of this input shall be class 3, i.e. $r=(0,0,1)$. Moreover, an arbitrary \n",
    "* weight-matrix W1 of size $(4\\times4)$ is generated. These are the weight of the first hidden layer, including the biases (first column)\n",
    "* weight-matrix W2 of size $(3\\times5)$ is generated. These are the weight of the output-layer, including the biases (first column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current input x=\n",
      "[1 1 2 3]\n",
      "Current target r=\n",
      "[0, 0, 1]\n",
      "Current matrix W1=\n",
      "[[-0.2  0.1  0.  -0.1]\n",
      " [ 0.3  0.4 -0.4  0.2]\n",
      " [ 0.4  0.1  0.3 -0.5]\n",
      " [ 0.  -0.5  0.4  0.1]]\n",
      "Current matrix W2=\n",
      "[[-0.3 -0.5  0.  -0.3  0.1]\n",
      " [-0.2  0.2 -0.5  0.4 -0.5]\n",
      " [-0.2 -0.3 -0.2 -0.4 -0.2]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "x=np.array([1,1,2,3]) # Training-Element with d=3 features plus the bias x_0=1 (first element in this vector)\n",
    "print(\"Current input x=\")\n",
    "print(x)\n",
    "r=[0,0,1] #Current class is 3. \n",
    "print(\"Current target r=\")\n",
    "print(r)\n",
    "W1=0.1*(np.random.randint(0,10,(4,4))-5) #Assumed current weight-matrix W1. First column refers to biases in layer 1\n",
    "print(\"Current matrix W1=\")\n",
    "print(W1)\n",
    "W2=0.1*(np.random.randint(0,10,(3,5))-5) #Assumed current weight-matrix W2. first column referst to biases in output layer\n",
    "print(\"Current matrix W2=\")\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward-Pass:\n",
    "Calculate for the given input the current output of the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current output at layer 1 h1=\n",
      "[0.4013 0.6225 0.4013 0.6457]\n"
     ]
    }
   ],
   "source": [
    "in1=np.dot(W1,x)\n",
    "h1=sigmoid(in1)\n",
    "print(\"Current output at layer 1 h1=\")\n",
    "print(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.     0.4013 0.6225 0.4013 0.6457]\n",
      "Current output at otuput layer:\n",
      "y = h2=\n",
      "[0.357  0.3441 0.2988]\n"
     ]
    }
   ],
   "source": [
    "h1ext=np.append([1],h1) # input to layer 2 is output of layer 1 + the constant 1 for the bias\n",
    "print(h1ext)\n",
    "in2=np.dot(W2,h1ext)\n",
    "y=h2=softmax(in2)\n",
    "print(\"Current output at otuput layer:\\ny = h2=\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contribution to Loss Function\n",
    "The current contribution of this training-element to the cross-entropy error-function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.74270956764372\n"
     ]
    }
   ],
   "source": [
    "print(crossEntropy(r,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass\n",
    "Next, the weight-matrices are adapted according to Stochastic Gradient Descent and the Backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current error vector at output:\n",
      " Delta=\n",
      "[[-0.357 ]\n",
      " [-0.3441]\n",
      " [ 0.7012]]\n",
      "Weight adaptation for otuput layer:\n",
      " dW2=\n",
      "[[-0.0357 -0.0143 -0.0222 -0.0143 -0.0231]\n",
      " [-0.0344 -0.0138 -0.0214 -0.0138 -0.0222]\n",
      " [ 0.0701  0.0281  0.0436  0.0281  0.0453]]\n"
     ]
    }
   ],
   "source": [
    "learnrate=0.1\n",
    "Delta=np.transpose(np.atleast_2d(r-y))\n",
    "print(\"Current error vector at output:\\n Delta=\")\n",
    "print(Delta)\n",
    "dW2=learnrate*np.dot(Delta,np.atleast_2d(h1ext))\n",
    "print(\"Weight adaptation for otuput layer:\\n dW2=\")\n",
    "print(dW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2403]\n",
      " [0.235 ]\n",
      " [0.2403]\n",
      " [0.2288]]\n",
      "Current matrix D1:\n",
      " D1=\n",
      "[[-0.0242]\n",
      " [ 0.0075]\n",
      " [-0.0747]\n",
      " [-0.0009]]\n",
      "Weight adaptation for hidden layer 1:\n",
      " dW1=\n",
      "[[-0.0024 -0.0024 -0.0048 -0.0073]\n",
      " [ 0.0007  0.0007  0.0015  0.0022]\n",
      " [-0.0075 -0.0075 -0.0149 -0.0224]\n",
      " [-0.0001 -0.0001 -0.0002 -0.0003]]\n"
     ]
    }
   ],
   "source": [
    "print(np.transpose(np.atleast_2d(sigmoid_prime(in1))))\n",
    "D1=np.dot(np.transpose(W2[:,1:]),Delta)*np.transpose(np.atleast_2d(sigmoid_prime(in1)))#Note that the first column from W2 (biases) have to be excluded!\n",
    "print(\"Current matrix D1:\\n D1=\")\n",
    "print(D1)\n",
    "dW1=learnrate*np.dot(D1,np.atleast_2d(x))\n",
    "print(\"Weight adaptation for hidden layer 1:\\n dW1=\")\n",
    "print(dW1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New weight matrix in hidden layer:\n",
      " W1=\n",
      "[[-0.2024  0.0976 -0.0048 -0.1073]\n",
      " [ 0.3007  0.4007 -0.3985  0.2022]\n",
      " [ 0.3925  0.0925  0.2851 -0.5224]\n",
      " [-0.0001 -0.5001  0.3998  0.0997]]\n",
      "New weight matrix in output layer:\n",
      " W2=\n",
      "[[-0.3357 -0.5143 -0.0222 -0.3143  0.0769]\n",
      " [-0.2344  0.1862 -0.5214  0.3862 -0.5222]\n",
      " [-0.1299 -0.2719 -0.1564 -0.3719 -0.1547]]\n"
     ]
    }
   ],
   "source": [
    "newW1=W1+dW1\n",
    "print(\"New weight matrix in hidden layer:\\n W1=\")\n",
    "print(newW1)\n",
    "newW2=W2+dW2\n",
    "print(\"New weight matrix in output layer:\\n W2=\")\n",
    "print(newW2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward-Pass with adapted weights\n",
    "Now, the adapted MLP's output for the same input vector is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current output at layer 1 after weight adaptation h1=\n",
      "[0.3926 0.6251 0.3747 0.6454]\n"
     ]
    }
   ],
   "source": [
    "in1=np.dot(newW1,x)\n",
    "h1=sigmoid(in1)\n",
    "print(\"Current output at layer 1 after weight adaptation h1=\")\n",
    "print(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current output of output layer after weight adaptation:\n",
      "y = h2=\n",
      "[0.3346 0.3151 0.3503]\n"
     ]
    }
   ],
   "source": [
    "h1ext=np.append([1],h1)\n",
    "in2=np.dot(newW2,h1ext)\n",
    "y=h2=softmax(in2)\n",
    "print(\"Current output of output layer after weight adaptation:\\ny = h2=\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Contribution to Loss Function\n",
    "As can be seen, the new output has a lower contribution to the cross-entropy error-function, than before the weight-adaptation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.513162303907582\n"
     ]
    }
   ],
   "source": [
    "print(crossEntropy(r,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP) for handwritten digit recognition\n",
    "\n",
    "This example demonstrates the application of a MLP classifier to recognize handwritten digits between 0 and 9. Each handwritten digit is represented as a $8 \\times 8$-greyscale image of 4 Bit depth. An image displaying digit $i$ is labeled by the class index $i$ with $i \\in \\lbrace 0,1,2,\\ldots,9\\rbrace$. The entire dataset contains 1797 labeled images. This dataset is often applied as a benchmark for evaluating and comparing machine learning algorithms. The dataset is available from different sources. E.g. it is contained in the *scikits-learn* datasets directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class MLP\n",
    "This class implements a Multilayer Perceptron (MLP) with Stochastic Gradient Descent (SGD) learning.\n",
    "Gradients are calculated using backpropagation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T17:12:17.782000Z",
     "start_time": "2017-12-06T18:12:17.629000+01:00"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "\n",
    "    def __init__(self, layerlist):\n",
    "        \"\"\"The list ``layerlist`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.\"\"\"\n",
    "        \n",
    "        self.num_layers = len(layerlist)\n",
    "        self.layerlist = layerlist\n",
    "        self.biases = [np.random.randn(y, 1) for y in layerlist[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(layerlist[:-1], layerlist[1:])]\n",
    "        self.testCorrect=[]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        #softmax activation in output-layer    \n",
    "        b=self.biases[-1]\n",
    "        w=self.weights[-1]    \n",
    "        a = softmax(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                numCorrect=self.evaluate(test_data)\n",
    "                self.testCorrect.append(numCorrect)\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j, numCorrect, n_test))     \n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        b=self.biases[-1]\n",
    "        w=self.weights[-1]    \n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = softmax(z)\n",
    "        activations.append(activation)\n",
    "        # Weight adaptations in output layer\n",
    "        delta = self.cost_derivative(activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # Weight adaptations in all other layers\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labeled data\n",
    "\n",
    "The image _handwritten digits_ dataset is loaded from the scikits-learn *datasets* directory. The *load_digits()* function returns a so called *Bunch*, which contains 2 numpy arrays - the images and the corresponding labeles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T17:12:29.016000Z",
     "start_time": "2017-12-06T18:12:22.856000+01:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Number of labeled images:  1797\n",
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "print(type(digits))\n",
    "print(type(digits.images))\n",
    "print(type(digits.target))\n",
    "print(\"Number of labeled images: \",n_samples)\n",
    "training_data=digits\n",
    "print(digits.images[0])\n",
    "print(digits.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the representation of the digits the first 4 images are dispayed in a *matplotlib*-figure. Moreover, the contents of the first image are printed. Each image is a $8 \\times 8$-numpy array with integer entries between $0$ (white) and $15$ (black). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T17:12:38.395000Z",
     "start_time": "2017-12-06T18:12:37.509000+01:00"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsEAAADACAYAAADlcr74AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAXyklEQVR4nO3dfZBddX3H8c+HhCgKIavEJ4Is6wNKxyHAqlUcB2VjEcXNVInaYg0dG2YqFtSOCU6toWNbopaCD6Mg+NCCDzzY6PhQSwrBZ2UX1yrGWPKABAhsdAOIWAS+/eOcyGbZ3Xt293fvPb+c92tmZ+7Dud/73XM/e/d7zz33XEeEAAAAgCbZr9sNAAAAAJ3GEAwAAIDGYQgGAABA4zAEAwAAoHEYggEAANA4DMEAAABonK4Pwbbn2f6N7aenXDZHtgdsb29T7fNs/8r2jhne7tu2V7ajp9yR3UeQ3byQ3UeQ3XyQ20eQ2zRmPASXodrz87Dt+8ed//OZ1ouIhyLiwIj4Zcpl8QjbR0j6G0lHRsSSbvczE7ZfYXuz7d/avnYuT2hkNz+5Ztf2Abavtn2L7bD9kjnWI7uZyTi7L7a9wfavbY/a/oLtJ8+yFrnNTMa5fZ7tYdtjZXb/y/ZzWt1uxkNwGaoDI+JASb+UdMq4yy6fpLH5M70PJHe4pLsiYle3G5mJ8on3KknnSHqipBFJn51tPbKbpSyzKykkfVPSn0kanXMxspujXLPbI+ljKvrvlfQ7SZfOphC5zVKuud0h6bWSniBpsaSvq8K8kHx3CNvvK185fs72vZJOs/0i29+3vdv2HbY/ZHv/cvn55ZaS3vL8ZeX1X7d9r+3vla9MZrRsef0rbf/C9t22P2z7O1Ntqrf9x7ZvtH2P7Tttf6C8fD/bV9neWfa/0fZzx93uMtsfsf2N8tXtN20/uby/3bY32T563PI7bK8uLx+zfantx0zR0xLb/1G+Gt9m+63TrPdFZS+jtrfbPseFk1SE4ellf5dMcfs/tT1S/v43237FJMs8y/Z1Lt4m2WX7320fPO76d9u+vazxc9snTLduK3itpJGI+GJE3C9praTn235mxdvPCNklu6myGxG/i4gLI+I7kh6ucpu5ILtkN2F2vxoRV0fEvRFxn6SPSjq+ym1nityS24S5HYuI7VF8DbJVPO+2nhUiYtY/krZLGphw2fskPSDpFBVD9gGSni/phZLmS+qT9AtJZ5bLz1ex1aS3PH+ZpF2S+iXtL+kLki6bxbJPknSvpMHyundI+r2klVP8LjdIemN5+iBJLyxP7ydpZXnZYyV9RNLQuNtdJukuSceU118vaZuKLUDzJJ0n6Zpxy++Q9D+Slkg6RNL3Ja0trxuQtL08PU/Fls93S1pQPpjbJZ04Rf+flfTFss8+STdLevPEulPc9sWSdks6sfx9D1PxVogkfXvPOpP07HKZBeX6/Y6kD5bX/ZGkWyQ9pTx/hKS+6dZtef4mSSum6Oujkj484bKfSxqcS27JLtlVm7M7ocedkl4y18ySXbKrDme3XPZvJX2b3JLbuue2XA+7JT1U/pzTMpdtCvW1Ff6orpwmqB8ft+xrJP10Fsv+paRvjbvOku7Q1KH+rqS/l/TEFr0fUvbw+HE9fGzc9W+X9JNx54+RtGtCqN8yoefNk4T6eElbJ9z3eyR9YpKe9pf0oKRnj7vsrZI2VAz1pZI+MMV1fwj1JNe9TtIN5ekjJd1Zhn7+bNbtJPU/I+l9Ey77gaTT5pJbskt2253dCTU6NQSTXbKbOrvHSBqT9GJyS24zyu2Bks6UdFKrZdt1dIhbx5+x/RzbXy3fIrhH0j+oCMdUdo47/VsVv9BMl33a+D6iWDPTfdLxdElHSdps+4e2Ty57n2f7/ba3lr3fXC4/vv87x52+f5LzE/sfv35uKXud6HAVb0ns3vMj6V2SnjLJsk9S8Qrolgl1D51k2ckcJmlLq4VsP8X2FbZvK9fFp1Wuh4jYLOmdKh7bu8q3t/b0Oum6reA3khZOuGyhilfs7UJ29z5PdmeX3W4gu3ufJ7tzyK7tZ0v6qqS3RsR3Z3LbGSK3e58nt3N8zo2I30j6uKTP2n7idMu2awiOCecvkvRTSc+MiIUqpny36b73uEPFWwiSJNvWNA9yRGyOiDeoCMi/SLra9mMl/YWkkyW9XNLBemQfk7n0f9i400+XdPsky9wq6X8jYtG4n4Mi4pRJlr1Lxab/wyfUva1iP7dKekaF5dZJ+j9Jzysfx5Uatx4i4rKIOF7FWxvzJP1zeflU67aVmySN3z/qoLL2TRVuO1tkd3pkt1p2u4HsTo/sVsyui31lN0h6b0TM+sPIFZHb6ZHb2T3n7qfiBcVkLxr2WqgTDpJ0t6T7XOwkfkYH7vMrko61fYqLT5yepeITg5Oy/Sbbh0TEw2WvoWLH6oNUPJC/kvQ4Sf+YoLczbR9avkI5R8W+SRN9T9IDtt9p+7HlK8zn2T5u4oIR8XsVR1H4J9sHlk9gb1fx1ksVl0p6i+2Xudixf4ntIydZ7iBJ90m62/ZhKt6mkiTZfm55+8eoeDV7v4o/tOnWbStXS1pqe3n5R/BeFftX3dzidimR3b2R3YofdLP9mHFP3gu6MDyT3b2R3QrZLe/jWknnR8QnKv4uKZHbvZHbarn9E9tHl7/7Qkn/qmLg3zzd7To1BL9T0ptVvI19kSZ/EJOKiDslvV7S+SoC+QxJP1IR0MmcLGmTi0+oflDS6yPiAUmfUvHK63YVWyBTvC30ORWvsreoeID+aZL+Hyx7eoGKfal2qVh3E3cP2OOvVXzAYJuKne0/I+nfqjRTvtX1V5I+pCJ012nvV597vLfs525JX1YxpO7xGEnvL/vcqeIwO39XXjfVupWLYwC/foq+7pS0oqw7JulYFR8g6CSyuzeyWyG7pS0qntyfLOm/Jd1vu5PH3SS7eyO71bK7SsWh0d7nR47pu7vK75QIud0bua2W2x5JV5T3t0XFlu6T9tx2Ki53It7n2Z6nIpivi4hvdbGPHSo+2LWxWz0gL2QXuSK7yBG5bY6uf21yO9k+yfbB5Sb396j4ROQPu9wW0BLZRa7ILnJEbptpnx6CJb1E0lYVm9xPkrQ8IqZ6ewOoE7KLXJFd5IjcNlBjdocAAAAA9tjXtwQDAAAAjzK/HUUPOeSQ6O3tbUfpWRkbG0tWa8eO6Y6fXd3ChVN9aHNmlixJ82HzefPmJamTyvbt27Vr1652HxtyL3XLbUqbN097lJjKHnrooSR1nva0aQ/dWNmiRYuS1ElpeHh4V0RMeXildtiXs3vvvWm+G2fLlpbH96/kgAMOSFLnyCMnO6pU93TjOVeqX3Z37tzZeqGKbrut6qF3p7dgwYIkdY466qgkdfaleaEtQ3Bvb6+GhobaUXpWrrzyymS1Vq9enaTOsmXLktQ577zzktTp6elJUieV/v7+jt9n3XKb0gknnJCkzu7daY6UdO655yapMzg4mKROSrZvab1UWvtydjdu3JikzvLly5PUWbp0aZI6qX6vVLrxnCvVL7vr1q1LVmvNmjVJ6hx6aNUvc5vetddem6TOvjQvsDsEAAAAGochGAAAAI3DEAwAAIDGYQgGAABA41QagstvUtls+2bbafb0BjqA7CJH5Ba5IrvIScshuPwO7Y9KeqWkoyS90Xaa42wAbUR2kSNyi1yRXeSmypbgF0i6OSK2RsQDkj4vqX7HJQIejewiR+QWuSK7yEqVIfhQSbeOO7+jvGwvtlfZHrI9NDo6mqo/YC5aZpfcooZ4zkWuyC6yUmUInuxbOOJRF0RcHBH9EdG/eHFHvywJmErL7JJb1BDPucgV2UVWqgzBOyQdNu78Ekm3t6cdICmyixyRW+SK7CIrVYbgGyQ9y/YRthdIeoOkL7e3LSAJsosckVvkiuwiK/NbLRARD9o+U9I3JM2T9MmIuKntnQFzRHaRI3KLXJFd5KblECxJEfE1SV9rcy9AcmQXOSK3yBXZRU74xjgAAAA0DkMwAAAAGochGAAAAI1TaZ/g3K1evTpZrW3btiWpMzY2lqTOE57whCR1rrjiiiR1JOnUU09NVgtpLFq0KEmd66+/Pkmd6667LkmdwUG+jKqORkZGktV62ctelqTOwQcfnKTO9u3bk9RBWmvWrElSJ+X/wosuuihJnTPOOCNJneHh4SR1BgYGktSpA7YEAwAAoHEYggEAANA4DMEAAABoHIZgAAAANA5DMAAAABqHIRgAAACNwxAMAACAxmEIBgAAQOMwBAMAAKBxGIIBAADQOAzBAAAAaByGYAAAADQOQzAAAAAahyEYAAAAjcMQDAAAgMZhCAYAAEDjMAQDAACgceZ3u4HpDA8PJ6mzbdu2JHUkacuWLUnq9PX1JamzbNmyJHVSrWtJOvXUU5PVarKRkZFktTZu3JisVgpLly7tdgtoo/Xr1yerdfTRRyeps3z58iR1zj333CR1kNaqVauS1Fm9enWSOpJ03HHHJalzxBFHJKkzMDCQpM6+hC3BAAAAaByGYAAAADQOQzAAAAAahyEYAAAAjcMQDAAAgMZhCAYAAEDjtByCbR9m+zrbm2zfZPusTjQGzBXZRY7ILXJFdpGbKscJflDSOyPiRtsHSRq2fU1E/KzNvQFzRXaRI3KLXJFdZKXlluCIuCMibixP3ytpk6RD290YMFdkFzkit8gV2UVuZrRPsO1eScdI+sEk162yPWR7aHR0NE13QCJTZZfcos54zkWuyC5yUHkItn2gpKslnR0R90y8PiIujoj+iOhfvHhxyh6BOZkuu+QWdcVzLnJFdpGLSkOw7f1VBPryiPhie1sC0iG7yBG5Ra7ILnJS5egQlnSppE0RcX77WwLSILvIEblFrsguclNlS/Dxkt4k6eW2R8qfk9vcF5AC2UWOyC1yRXaRlZaHSIuIb0tyB3oBkiK7yBG5Ra7ILnLDN8YBAACgcRiCAQAA0DgMwQAAAGicKl+b3DVjY2NJ6hx77LFJ6khSX19fslopHHfccd1uARNccMEFSeqsXbs2SR1Juvvuu5PVSuGEE07odgtoo7PPPjtZrd7e3iR1UvU0ODiYpA7SSvW/eevWrUnqSNK2bduS1BkYGEhSJ9VM1dPTk6ROHbAlGAAAAI3DEAwAAIDGYQgGAABA4zAEAwAAoHEYggEAANA4DMEAAABoHIZgAAAANA5DMAAAABqHIRgAAACNwxAMAACAxmEIBgAAQOMwBAMAAKBxGIIBAADQOAzBAAAAaByGYAAAADQOQzAAAAAahyEYAAAAjTO/2w1MZ2xsLEmdZcuWJalTR6nWUU9PT5I6kM4+++wkdVauXJmkjlS/x3f37t3dbgGTSPW4XHDBBUnqSNL69euT1Urh05/+dLdbQBv19fUlq/XrX/86SZ2BgYFa1dmwYUOSOlL3/zexJRgAAACNwxAMAACAxmEIBgAAQOMwBAMAAKBxGIIBAADQOJWHYNvzbP/I9lfa2RCQErlFrsguckV2kYuZbAk+S9KmdjUCtAm5Ra7ILnJFdpGFSkOw7SWSXiXpkva2A6RDbpErsotckV3kpOqW4AskvUvSw1MtYHuV7SHbQ6Ojo0maA+aI3CJXZBe5IrvIRssh2ParJd0VEcPTLRcRF0dEf0T0L168OFmDwGyQW+SK7CJXZBe5qbIl+HhJr7G9XdLnJb3c9mVt7QqYO3KLXJFd5IrsIisth+CIOCcilkREr6Q3SLo2Ik5re2fAHJBb5IrsIldkF7nhOMEAAABonPkzWTgiNkra2JZOgDYht8gV2UWuyC5ywJZgAAAANA5DMAAAABqHIRgAAACNM6N9gjutp6cnSZ3h4WkPWdgVY2NjSeoMDQ0lqbNixYokdYAqRkZGktRZunRpkjoorF27NkmdCy+8MEmdlNavX5+kzqJFi5LUwb4v1QyzYcOGJHXOOOOMJHXWrVuXpI4knXfeeclqzQZbggEAANA4DMEAAABoHIZgAAAANA5DMAAAABqHIRgAAACNwxAMAACAxmEIBgAAQOMwBAMAAKBxGIIBAADQOAzBAAAAaByGYAAAADQOQzAAAAAahyEYAAAAjcMQDAAAgMZhCAYAAEDjMAQDAACgcRiCAQAA0Djzu93AdPr6+pLUGRoaSlJHkq688spa1Ull9erV3W4BQJetXLkySZ2NGzcmqSNJP/7xj5PUWb58eZI6g4ODSeqcfvrpSepI6XqCtGbNmmS1BgYGktQZGxtLUueaa65JUmfFihVJ6tQBW4IBAADQOAzBAAAAaByGYAAAADQOQzAAAAAahyEYAAAAjVNpCLa9yPZVtn9ue5PtF7W7MSAFsosckVvkiuwiJ1UPkXahpP+MiNfZXiDpcW3sCUiJ7CJH5Ba5IrvIRssh2PZCSS+VtFKSIuIBSQ+0ty1g7sguckRukSuyi9xU2R2iT9KopE/Z/pHtS2w/vs19ASmQXeSI3CJXZBdZqTIEz5d0rKSPRcQxku6T9KivVLG9yvaQ7aHR0dHEbQKz0jK75BY1xHMuckV2kZUqQ/AOSTsi4gfl+atUhHwvEXFxRPRHRP/ixYtT9gjMVsvsklvUEM+5yBXZRVZaDsERsVPSrbaPLC86UdLP2toVkADZRY7ILXJFdpGbqkeHeJuky8tPem6VdHr7WgKSIrvIEblFrsguslFpCI6IEUn9be4FSI7sIkfkFrkiu8gJ3xgHAACAxmEIBgAAQOMwBAMAAKBxGIIBAADQOFWPDtEVfX19SeqsW7cuSR1JWr16dZI6/f1pPjcwPDycpA7qZ9GiRclqDQ4OJqnzpS99KUmdjRs3JqmzcuXKJHVQWLp0aZI6IyMjSeqkrLV27dokdVL9DfT29iapI6X7+4bU09OTrNaqVauS1UphxYoVSepcdNFFSerUAVuCAQAA0DgMwQAAAGgchmAAAAA0DkMwAAAAGochGAAAAI3DEAwAAIDGYQgGAABA4zAEAwAAoHEYggEAANA4DMEAAABoHIZgAAAANA5DMAAAABqHIRgAAACNwxAMAACAxmEIBgAAQOMwBAMAAKBxGIIBAADQOI6I9EXtUUm3tFjsEEm7kt/57NFPa53s6fCIWNyh+5KUbW6l+vVUt34ksivV73GpWz9S/Xrap3Mrkd1E6taPlEl22zIEV7pjeygi+rty55Ogn9bq2FOn1XEd1K2nuvUj1bOnTqvbOqhbP1L9eqpbP91St/VAP63VsafJsDsEAAAAGochGAAAAI3TzSH44i7e92Top7U69tRpdVwHdeupbv1I9eyp0+q2DurWj1S/nurWT7fUbT3QT2t17OlRurZPMAAAANAt7A4BAACAxmEIBgAAQON0fAi2fZLtzbZvtr2m0/c/ST+H2b7O9ibbN9k+q9s9SZLtebZ/ZPsrNehlke2rbP+8XE8v6nZP3VCn7JLbashugey2Rnbrp065Lfshu617ySq3Hd0n2PY8Sb+QtEzSDkk3SHpjRPysY008uqenSnpqRNxo+yBJw5KWd7Onsq93SOqXtDAiXt3lXj4j6VsRcYntBZIeFxG7u9lTp9Utu+S2cj9kl+xW7Yvs1kjdclv2RHZb95JVbju9JfgFkm6OiK0R8YCkz0sa7HAPe4mIOyLixvL0vZI2STq0mz3ZXiLpVZIu6WYfZS8LJb1U0qWSFBEP1DnQbVSr7JLb1sjuH5DdFshuLdUqtxLZrdBLdrnt9BB8qKRbx53foS4HaDzbvZKOkfSD7naiCyS9S9LDXe5DkvokjUr6VPl2yyW2H9/tprqgttklt1MiuwWy2xrZrZ/a5lYiu1PILredHoI9yWW1OEab7QMlXS3p7Ii4p4t9vFrSXREx3K0eJpgv6VhJH4uIYyTdJ6nr+2Z1QS2zS26nRXYLZHf6PshuPdUytxLZnUZ2ue30ELxD0mHjzi+RdHuHe3gU2/urCPTlEfHFLrdzvKTX2N6u4u2fl9u+rIv97JC0IyL2vNq9SkXIm6Z22SW3LZHdAtmdHtmtp9rlViK7LWSX204PwTdIepbtI8odpt8g6csd7mEvtq1i/5VNEXF+N3uRpIg4JyKWRESvivVzbUSc1sV+dkq61faR5UUnSurqhwC6pFbZJbeVeiK7BbI7DbJbW7XKrUR2K/STXW7nd/LOIuJB22dK+oakeZI+GRE3dbKHSRwv6U2SfmJ7pLzs3RHxtS72VDdvk3R5+US0VdLpXe6n42qYXXJbDdklu7lqdHZrmFuJ7FaRVW752mQAAAA0Dt8YBwAAgMZhCAYAAEDjMAQDAACgcRiCAQAA0DgMwQAAAGgchmAAAAA0DkMwAAAAGuf/ATOawzH7mKGjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "NIMAGES=4\n",
    "for index in range(NIMAGES):\n",
    "    plt.subplot(1,NIMAGES, index+1)\n",
    "    plt.imshow(digits.images[index,:], cmap=plt.cm.gray_r)\n",
    "    plt.title('Training sample of class: %i' % digits.target[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split labeled dataset into training- and test-partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T17:13:00.641000Z",
     "start_time": "2017-12-06T18:13:00.625000+01:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "NUMTRAIN=1000\n",
    "training_inputs = [np.reshape(x, (64, 1)) for x in digits.images[:NUMTRAIN]]\n",
    "training_results = [oneToMany(y) for y in digits.target[:NUMTRAIN]]\n",
    "training_data = list(zip(training_inputs, training_results))\n",
    "#test_data = zip(test_inputs, test_results)\n",
    "test_inputs = [np.reshape(x, (64, 1)) for x in digits.images[NUMTRAIN:]]\n",
    "test_data = list(zip(test_inputs, digits.target[NUMTRAIN:]))\n",
    "print(type(training_data[0][0]))\n",
    "print(len(training_data))\n",
    "#print training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate, configure, train and test MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T17:14:14.633000Z",
     "start_time": "2017-12-06T18:13:13.667000+01:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 113 / 797\n",
      "Epoch 1: 180 / 797\n",
      "Epoch 2: 188 / 797\n",
      "Epoch 3: 240 / 797\n",
      "Epoch 4: 254 / 797\n",
      "Epoch 5: 227 / 797\n",
      "Epoch 6: 291 / 797\n",
      "Epoch 7: 301 / 797\n",
      "Epoch 8: 311 / 797\n",
      "Epoch 9: 321 / 797\n",
      "Epoch 10: 355 / 797\n",
      "Epoch 11: 341 / 797\n",
      "Epoch 12: 361 / 797\n",
      "Epoch 13: 385 / 797\n",
      "Epoch 14: 395 / 797\n",
      "Epoch 15: 435 / 797\n",
      "Epoch 16: 438 / 797\n",
      "Epoch 17: 440 / 797\n",
      "Epoch 18: 466 / 797\n",
      "Epoch 19: 467 / 797\n",
      "Epoch 20: 476 / 797\n",
      "Epoch 21: 489 / 797\n",
      "Epoch 22: 507 / 797\n",
      "Epoch 23: 496 / 797\n",
      "Epoch 24: 504 / 797\n",
      "Epoch 25: 498 / 797\n",
      "Epoch 26: 518 / 797\n",
      "Epoch 27: 529 / 797\n",
      "Epoch 28: 534 / 797\n",
      "Epoch 29: 536 / 797\n",
      "Epoch 30: 540 / 797\n",
      "Epoch 31: 555 / 797\n",
      "Epoch 32: 556 / 797\n",
      "Epoch 33: 552 / 797\n",
      "Epoch 34: 563 / 797\n",
      "Epoch 35: 562 / 797\n",
      "Epoch 36: 557 / 797\n",
      "Epoch 37: 565 / 797\n",
      "Epoch 38: 574 / 797\n",
      "Epoch 39: 579 / 797\n",
      "Epoch 40: 578 / 797\n",
      "Epoch 41: 584 / 797\n",
      "Epoch 42: 582 / 797\n",
      "Epoch 43: 594 / 797\n",
      "Epoch 44: 594 / 797\n",
      "Epoch 45: 585 / 797\n",
      "Epoch 46: 592 / 797\n",
      "Epoch 47: 593 / 797\n",
      "Epoch 48: 599 / 797\n",
      "Epoch 49: 604 / 797\n",
      "Epoch 50: 604 / 797\n",
      "Epoch 51: 610 / 797\n",
      "Epoch 52: 614 / 797\n",
      "Epoch 53: 609 / 797\n",
      "Epoch 54: 610 / 797\n",
      "Epoch 55: 610 / 797\n",
      "Epoch 56: 618 / 797\n",
      "Epoch 57: 616 / 797\n",
      "Epoch 58: 619 / 797\n",
      "Epoch 59: 619 / 797\n",
      "Epoch 60: 622 / 797\n",
      "Epoch 61: 625 / 797\n",
      "Epoch 62: 625 / 797\n",
      "Epoch 63: 633 / 797\n",
      "Epoch 64: 625 / 797\n",
      "Epoch 65: 626 / 797\n",
      "Epoch 66: 631 / 797\n",
      "Epoch 67: 627 / 797\n",
      "Epoch 68: 637 / 797\n",
      "Epoch 69: 640 / 797\n",
      "Epoch 70: 635 / 797\n",
      "Epoch 71: 638 / 797\n",
      "Epoch 72: 640 / 797\n",
      "Epoch 73: 643 / 797\n",
      "Epoch 74: 638 / 797\n",
      "Epoch 75: 646 / 797\n",
      "Epoch 76: 648 / 797\n",
      "Epoch 77: 648 / 797\n",
      "Epoch 78: 645 / 797\n",
      "Epoch 79: 640 / 797\n",
      "Epoch 80: 642 / 797\n",
      "Epoch 81: 645 / 797\n",
      "Epoch 82: 644 / 797\n",
      "Epoch 83: 641 / 797\n",
      "Epoch 84: 642 / 797\n",
      "Epoch 85: 637 / 797\n",
      "Epoch 86: 643 / 797\n",
      "Epoch 87: 642 / 797\n",
      "Epoch 88: 645 / 797\n",
      "Epoch 89: 643 / 797\n",
      "Epoch 90: 639 / 797\n",
      "Epoch 91: 645 / 797\n",
      "Epoch 92: 639 / 797\n",
      "Epoch 93: 645 / 797\n",
      "Epoch 94: 645 / 797\n",
      "Epoch 95: 640 / 797\n",
      "Epoch 96: 641 / 797\n",
      "Epoch 97: 641 / 797\n",
      "Epoch 98: 640 / 797\n",
      "Epoch 99: 640 / 797\n",
      "Epoch 100: 644 / 797\n",
      "Epoch 101: 641 / 797\n",
      "Epoch 102: 642 / 797\n",
      "Epoch 103: 640 / 797\n",
      "Epoch 104: 641 / 797\n",
      "Epoch 105: 640 / 797\n",
      "Epoch 106: 640 / 797\n",
      "Epoch 107: 642 / 797\n",
      "Epoch 108: 639 / 797\n",
      "Epoch 109: 641 / 797\n",
      "Epoch 110: 640 / 797\n",
      "Epoch 111: 643 / 797\n",
      "Epoch 112: 640 / 797\n",
      "Epoch 113: 639 / 797\n",
      "Epoch 114: 643 / 797\n",
      "Epoch 115: 641 / 797\n",
      "Epoch 116: 641 / 797\n",
      "Epoch 117: 639 / 797\n",
      "Epoch 118: 639 / 797\n",
      "Epoch 119: 642 / 797\n",
      "Epoch 120: 644 / 797\n",
      "Epoch 121: 639 / 797\n",
      "Epoch 122: 641 / 797\n",
      "Epoch 123: 644 / 797\n",
      "Epoch 124: 640 / 797\n",
      "Epoch 125: 644 / 797\n",
      "Epoch 126: 644 / 797\n",
      "Epoch 127: 645 / 797\n",
      "Epoch 128: 643 / 797\n",
      "Epoch 129: 646 / 797\n",
      "Epoch 130: 647 / 797\n",
      "Epoch 131: 643 / 797\n",
      "Epoch 132: 653 / 797\n",
      "Epoch 133: 647 / 797\n",
      "Epoch 134: 644 / 797\n",
      "Epoch 135: 646 / 797\n",
      "Epoch 136: 647 / 797\n",
      "Epoch 137: 647 / 797\n",
      "Epoch 138: 643 / 797\n",
      "Epoch 139: 647 / 797\n",
      "Epoch 140: 648 / 797\n",
      "Epoch 141: 649 / 797\n",
      "Epoch 142: 650 / 797\n",
      "Epoch 143: 651 / 797\n",
      "Epoch 144: 652 / 797\n",
      "Epoch 145: 648 / 797\n",
      "Epoch 146: 649 / 797\n",
      "Epoch 147: 649 / 797\n",
      "Epoch 148: 653 / 797\n",
      "Epoch 149: 651 / 797\n",
      "Epoch 150: 650 / 797\n",
      "Epoch 151: 652 / 797\n",
      "Epoch 152: 652 / 797\n",
      "Epoch 153: 652 / 797\n",
      "Epoch 154: 653 / 797\n",
      "Epoch 155: 651 / 797\n",
      "Epoch 156: 652 / 797\n",
      "Epoch 157: 649 / 797\n",
      "Epoch 158: 653 / 797\n",
      "Epoch 159: 651 / 797\n",
      "Epoch 160: 657 / 797\n",
      "Epoch 161: 654 / 797\n",
      "Epoch 162: 654 / 797\n",
      "Epoch 163: 652 / 797\n",
      "Epoch 164: 654 / 797\n",
      "Epoch 165: 653 / 797\n",
      "Epoch 166: 651 / 797\n",
      "Epoch 167: 656 / 797\n",
      "Epoch 168: 654 / 797\n",
      "Epoch 169: 655 / 797\n",
      "Epoch 170: 654 / 797\n",
      "Epoch 171: 659 / 797\n",
      "Epoch 172: 655 / 797\n",
      "Epoch 173: 651 / 797\n",
      "Epoch 174: 653 / 797\n",
      "Epoch 175: 654 / 797\n",
      "Epoch 176: 657 / 797\n",
      "Epoch 177: 656 / 797\n",
      "Epoch 178: 658 / 797\n",
      "Epoch 179: 661 / 797\n",
      "Epoch 180: 655 / 797\n",
      "Epoch 181: 659 / 797\n",
      "Epoch 182: 659 / 797\n",
      "Epoch 183: 659 / 797\n",
      "Epoch 184: 659 / 797\n",
      "Epoch 185: 656 / 797\n",
      "Epoch 186: 657 / 797\n",
      "Epoch 187: 661 / 797\n",
      "Epoch 188: 663 / 797\n",
      "Epoch 189: 658 / 797\n",
      "Epoch 190: 659 / 797\n",
      "Epoch 191: 661 / 797\n",
      "Epoch 192: 658 / 797\n",
      "Epoch 193: 661 / 797\n",
      "Epoch 194: 659 / 797\n",
      "Epoch 195: 661 / 797\n",
      "Epoch 196: 661 / 797\n",
      "Epoch 197: 661 / 797\n",
      "Epoch 198: 661 / 797\n",
      "Epoch 199: 660 / 797\n",
      "Epoch 200: 658 / 797\n",
      "Epoch 201: 660 / 797\n",
      "Epoch 202: 658 / 797\n",
      "Epoch 203: 664 / 797\n",
      "Epoch 204: 659 / 797\n",
      "Epoch 205: 661 / 797\n",
      "Epoch 206: 662 / 797\n",
      "Epoch 207: 661 / 797\n",
      "Epoch 208: 661 / 797\n",
      "Epoch 209: 660 / 797\n",
      "Epoch 210: 666 / 797\n",
      "Epoch 211: 661 / 797\n",
      "Epoch 212: 661 / 797\n",
      "Epoch 213: 664 / 797\n",
      "Epoch 214: 666 / 797\n",
      "Epoch 215: 665 / 797\n",
      "Epoch 216: 662 / 797\n",
      "Epoch 217: 662 / 797\n",
      "Epoch 218: 663 / 797\n",
      "Epoch 219: 663 / 797\n",
      "Epoch 220: 665 / 797\n",
      "Epoch 221: 666 / 797\n",
      "Epoch 222: 667 / 797\n",
      "Epoch 223: 667 / 797\n",
      "Epoch 224: 664 / 797\n",
      "Epoch 225: 664 / 797\n",
      "Epoch 226: 666 / 797\n",
      "Epoch 227: 664 / 797\n",
      "Epoch 228: 663 / 797\n",
      "Epoch 229: 665 / 797\n",
      "Epoch 230: 669 / 797\n",
      "Epoch 231: 667 / 797\n",
      "Epoch 232: 665 / 797\n",
      "Epoch 233: 667 / 797\n",
      "Epoch 234: 667 / 797\n",
      "Epoch 235: 666 / 797\n",
      "Epoch 236: 666 / 797\n",
      "Epoch 237: 668 / 797\n",
      "Epoch 238: 667 / 797\n",
      "Epoch 239: 668 / 797\n",
      "Epoch 240: 665 / 797\n",
      "Epoch 241: 667 / 797\n",
      "Epoch 242: 666 / 797\n",
      "Epoch 243: 669 / 797\n",
      "Epoch 244: 667 / 797\n",
      "Epoch 245: 667 / 797\n",
      "Epoch 246: 667 / 797\n",
      "Epoch 247: 669 / 797\n",
      "Epoch 248: 665 / 797\n",
      "Epoch 249: 664 / 797\n",
      "Epoch 250: 668 / 797\n",
      "Epoch 251: 669 / 797\n",
      "Epoch 252: 667 / 797\n",
      "Epoch 253: 666 / 797\n",
      "Epoch 254: 667 / 797\n",
      "Epoch 255: 670 / 797\n",
      "Epoch 256: 668 / 797\n",
      "Epoch 257: 666 / 797\n",
      "Epoch 258: 666 / 797\n",
      "Epoch 259: 669 / 797\n",
      "Epoch 260: 667 / 797\n",
      "Epoch 261: 669 / 797\n",
      "Epoch 262: 666 / 797\n",
      "Epoch 263: 667 / 797\n",
      "Epoch 264: 668 / 797\n",
      "Epoch 265: 669 / 797\n",
      "Epoch 266: 668 / 797\n",
      "Epoch 267: 668 / 797\n",
      "Epoch 268: 670 / 797\n",
      "Epoch 269: 670 / 797\n",
      "Epoch 270: 669 / 797\n",
      "Epoch 271: 667 / 797\n",
      "Epoch 272: 669 / 797\n",
      "Epoch 273: 667 / 797\n",
      "Epoch 274: 668 / 797\n",
      "Epoch 275: 669 / 797\n",
      "Epoch 276: 668 / 797\n",
      "Epoch 277: 668 / 797\n",
      "Epoch 278: 669 / 797\n",
      "Epoch 279: 668 / 797\n",
      "Epoch 280: 668 / 797\n",
      "Epoch 281: 666 / 797\n",
      "Epoch 282: 668 / 797\n",
      "Epoch 283: 667 / 797\n",
      "Epoch 284: 667 / 797\n",
      "Epoch 285: 666 / 797\n",
      "Epoch 286: 671 / 797\n",
      "Epoch 287: 666 / 797\n",
      "Epoch 288: 668 / 797\n",
      "Epoch 289: 666 / 797\n",
      "Epoch 290: 668 / 797\n",
      "Epoch 291: 668 / 797\n",
      "Epoch 292: 668 / 797\n",
      "Epoch 293: 666 / 797\n",
      "Epoch 294: 666 / 797\n",
      "Epoch 295: 667 / 797\n",
      "Epoch 296: 670 / 797\n",
      "Epoch 297: 669 / 797\n",
      "Epoch 298: 669 / 797\n",
      "Epoch 299: 671 / 797\n",
      "Epoch 300: 670 / 797\n",
      "Epoch 301: 672 / 797\n",
      "Epoch 302: 672 / 797\n",
      "Epoch 303: 672 / 797\n",
      "Epoch 304: 670 / 797\n",
      "Epoch 305: 671 / 797\n",
      "Epoch 306: 671 / 797\n",
      "Epoch 307: 669 / 797\n",
      "Epoch 308: 673 / 797\n",
      "Epoch 309: 671 / 797\n",
      "Epoch 310: 673 / 797\n",
      "Epoch 311: 672 / 797\n",
      "Epoch 312: 673 / 797\n",
      "Epoch 313: 673 / 797\n",
      "Epoch 314: 671 / 797\n",
      "Epoch 315: 672 / 797\n",
      "Epoch 316: 673 / 797\n",
      "Epoch 317: 673 / 797\n",
      "Epoch 318: 672 / 797\n",
      "Epoch 319: 671 / 797\n",
      "Epoch 320: 673 / 797\n",
      "Epoch 321: 674 / 797\n",
      "Epoch 322: 669 / 797\n",
      "Epoch 323: 672 / 797\n",
      "Epoch 324: 671 / 797\n",
      "Epoch 325: 672 / 797\n",
      "Epoch 326: 673 / 797\n",
      "Epoch 327: 671 / 797\n",
      "Epoch 328: 672 / 797\n",
      "Epoch 329: 671 / 797\n",
      "Epoch 330: 670 / 797\n",
      "Epoch 331: 673 / 797\n",
      "Epoch 332: 671 / 797\n",
      "Epoch 333: 673 / 797\n",
      "Epoch 334: 672 / 797\n",
      "Epoch 335: 673 / 797\n",
      "Epoch 336: 673 / 797\n",
      "Epoch 337: 673 / 797\n",
      "Epoch 338: 672 / 797\n",
      "Epoch 339: 673 / 797\n",
      "Epoch 340: 675 / 797\n",
      "Epoch 341: 673 / 797\n",
      "Epoch 342: 675 / 797\n",
      "Epoch 343: 672 / 797\n",
      "Epoch 344: 671 / 797\n",
      "Epoch 345: 673 / 797\n",
      "Epoch 346: 674 / 797\n",
      "Epoch 347: 673 / 797\n",
      "Epoch 348: 672 / 797\n",
      "Epoch 349: 673 / 797\n",
      "Epoch 350: 673 / 797\n",
      "Epoch 351: 673 / 797\n",
      "Epoch 352: 672 / 797\n",
      "Epoch 353: 674 / 797\n",
      "Epoch 354: 672 / 797\n",
      "Epoch 355: 675 / 797\n",
      "Epoch 356: 673 / 797\n",
      "Epoch 357: 673 / 797\n",
      "Epoch 358: 671 / 797\n",
      "Epoch 359: 673 / 797\n",
      "Epoch 360: 673 / 797\n",
      "Epoch 361: 673 / 797\n",
      "Epoch 362: 674 / 797\n",
      "Epoch 363: 672 / 797\n",
      "Epoch 364: 673 / 797\n",
      "Epoch 365: 673 / 797\n",
      "Epoch 366: 674 / 797\n",
      "Epoch 367: 674 / 797\n",
      "Epoch 368: 675 / 797\n",
      "Epoch 369: 674 / 797\n",
      "Epoch 370: 673 / 797\n",
      "Epoch 371: 673 / 797\n",
      "Epoch 372: 673 / 797\n",
      "Epoch 373: 672 / 797\n",
      "Epoch 374: 674 / 797\n",
      "Epoch 375: 673 / 797\n",
      "Epoch 376: 676 / 797\n",
      "Epoch 377: 673 / 797\n",
      "Epoch 378: 673 / 797\n",
      "Epoch 379: 673 / 797\n",
      "Epoch 380: 673 / 797\n",
      "Epoch 381: 675 / 797\n",
      "Epoch 382: 675 / 797\n",
      "Epoch 383: 673 / 797\n",
      "Epoch 384: 675 / 797\n",
      "Epoch 385: 673 / 797\n",
      "Epoch 386: 674 / 797\n",
      "Epoch 387: 674 / 797\n",
      "Epoch 388: 672 / 797\n",
      "Epoch 389: 674 / 797\n",
      "Epoch 390: 672 / 797\n",
      "Epoch 391: 672 / 797\n",
      "Epoch 392: 670 / 797\n",
      "Epoch 393: 674 / 797\n",
      "Epoch 394: 673 / 797\n",
      "Epoch 395: 672 / 797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396: 674 / 797\n",
      "Epoch 397: 672 / 797\n",
      "Epoch 398: 672 / 797\n",
      "Epoch 399: 673 / 797\n"
     ]
    }
   ],
   "source": [
    "net = MLP([64,30, 10])\n",
    "net.SGD(training_data, 400, 10, .03,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T17:14:14.633000Z",
     "start_time": "2017-12-06T18:13:13.667000+01:00"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if len(net.testCorrect)>0:\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.plot(range(len(net.testCorrect)),net.testCorrect)\n",
    "    plt.title(\"Number of correct classifications\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    #plt.hold(True)\n",
    "    plt.plot([0,len(net.testCorrect)],[n_samples-NUMTRAIN,n_samples-NUMTRAIN])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T17:14:22.840000Z",
     "start_time": "2017-12-06T18:14:22.840000+01:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.8444165621079046)\n"
     ]
    }
   ],
   "source": [
    "print((\"Accuracy: \",net.testCorrect[-1]/float((n_samples-NUMTRAIN))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
